# 【强化学习】04.策略梯度（Policy Gradient）算法原理

---

## **1.基本原理**
策略梯度（Policy Gradient）方法是一类直接基于策略优化的强化学习算法。它的核心思想是通过参数化一个策略函数（policy），直接对策略的参数进行优化，使得在特定环境中累积的期望回报最大化。与值函数方法（如Q-learning或DQN）不同，策略梯度方法不显式地学习状态值函数或动作值函数，而是直接学习一个参数化的概率分布，用于决定在给定状态下采取哪种动作。

在策略梯度方法中，策略 $\pi_{\theta}(a|s)$ 表示在状态 $s$ 下采取动作 $a$ 的概率，参数 $\theta$ 是策略的参数。通过一个优化目标（通常是期望累积回报 $J(\theta)$），我们使用梯度上升或下降更新参数，使得策略不断改进。

目标函数：
$$
J(\theta) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^T \gamma^t r_t \right]
$$

其中：
- $r_t$ 为时间步 $t$ 的奖励；
- $\gamma$ 是折扣因子，用于权衡短期与长期回报；
- $\pi_{\theta}$ 是当前策略。

策略梯度利用以下梯度公式进行优化：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) \cdot G_t \right]
$$

其中：
- $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ 表示从时间步 $t$ 开始的累积回报；
- $\nabla_{\theta} \log \pi_{\theta}(a|s)$ 是对策略的对数概率的梯度。

通过对 $\theta$ 进行优化，我们可以最大化策略的期望回报。

---

## **2.算法流程**
以下是策略梯度算法（如REINFORCE）的具体流程：

**(1)初始化**：
   - 初始化策略网络的参数 $\theta$；
   - 定义环境、优化器以及超参数（学习率、折扣因子等）。

**(2)采样数据**：
   - 从环境中采样完整的轨迹。即：从初始状态 $s_0$ 开始，按照当前策略 $\pi_{\theta}(a|s)$ 选择动作，获得下一个状态和奖励，直到终止状态。

**(3)计算回报**：
   - 对于每个时间步 $t$，计算从时间步 $t$ 开始的折扣累计回报 $G_t$。

**(4)计算损失并更新参数**：
   - 使用梯度估计公式，计算每个时间步的策略梯度 $\nabla_{\theta} \log \pi_{\theta}(a|s) \cdot G_t$；
   - 反向传播更新策略参数 $\theta$。

**(5)重复迭代**：
   - 根据新参数更新策略，继续采样和优化，直到达到指定的训练轮数或满足策略性能要求。

策略梯度方法是一种直接优化策略的强化学习算法，能够很好地解决连续动作问题和复杂策略建模问题。然而，与值函数方法（如Q-learning和DQN）相比，它的样本效率较低，训练时易受高方差影响。因此，在实际应用中，往往通过结合两者优点（如Actor-Critic方法）进一步提升性能。

---

## **3.优缺点分析**

### **A.优势**
- **直接优化策略**： 策略梯度方法直接优化策略而非值函数，可以很自然地处理连续动作空间，比Q-learning和DQN在连续动作场景下更适用。
- **策略多样性**： 策略梯度方法输出的是概率分布，允许随机性动作选择，适合解决需要探索的任务。
- **策略稳定性**： 不依赖于值函数的精度，避免了值函数方法（如Q-learning）中的过估计或欠估计问题。
- **适用于复杂策略**： 策略网络可以直接表示复杂策略（如混合策略或多模态策略），在高维空间中有更强的建模能力。

### **B.劣势**
- **高方差**： 策略梯度估计的梯度具有较高的方差，会导致训练过程不稳定。为此，引入了许多变种（如Actor-Critic、PPO）来改进。
- **样本效率低**： 策略梯度方法通常需要大量采样才能得到稳定的估计，训练成本较高。
- **不充分利用经验**： 每次训练只用当前采样的数据，没有像DQN一样重复利用经验回放（Replay Buffer）。
- **易出现局部最优**： 策略梯度方法可能陷入局部最优，尤其是在复杂环境中。

---

### **4.与Q-Learning和DQN的对比**

| **维度**         | **策略梯度**                           | **Q-Learning/DQN**                         |
|-------------------|-----------------------------------------|--------------------------------------------|
| **核心思想**     | 直接优化策略 $\pi(a\|s)$              |  学习状态-动作值函数 $Q(s, a)$             |
| **决策方式**     | 随机采样动作概率分布                  |  通过最大化 $Q(s, a)$ 决定最优动作          |
| **应用场景**     | 离散和连续动作空间                   | 主要用于离散动作空间                       |
| **样本效率**     | 样本效率低，仅使用当前采样数据          | 样本效率高，使用经验回放                   |
| **稳定性**       | 易受高方差影响，训练不稳定             | 通过目标网络、经验回放提高训练稳定性       |
| **探索与随机性** | 默认自带探索（随机采样动作概率分布）   | 需要特殊机制（如$\epsilon$-贪婪策略）      |
| **复杂策略**     | 能建模复杂策略（混合策略、多模态）     | 受限于值函数，难以表示复杂策略             |
| **实现难度**     | 实现较为简洁，但需注意数值稳定性       | 实现复杂（如目标网络、经验回放）           |
| **样例算法**     | REINFORCE、PPO、TRPO、Actor-Critic    | Q-Learning、DQN、Double DQN、Dueling DQN   |

---

## **5.游戏介绍**
在代码中，策略梯度算法被应用于 `CartPole-v0` 环境，这是一个经典的强化学习任务，目标是通过控制推力保持杆子直立。在该环境中：

- **状态空间**：由 4 个连续值构成，包括杆子的角度、角速度、小车位置和小车速度；

- **动作空间**：包含 2 个离散动作（向左或向右施加推力）；

- **奖励函数**：每个时间步杆子保持直立，奖励为 +1。

这种任务对算法的探索能力和状态空间建模能力提出了要求，策略梯度方法通过建模动作概率分布，能够高效地完成这一任务。




---

## **6.训练代码**
```python
# ------------- train.py -------------
import gym
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm



class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)
    

class REINFORCE:
    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,
                 device):
        self.policy_net = PolicyNet(state_dim, hidden_dim,
                                    action_dim).to(device)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(),
                                          lr=learning_rate)  # 使用Adam优化器
        self.gamma = gamma  # 折扣因子
        self.device = device


    def take_action(self, state):  # 根据动作概率分布随机采样
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.policy_net(state)   # 通过神经网络（策略网络 PolicyNet）将状态映射为动作概率分布 πθ(a∣s)
        action_dist = torch.distributions.Categorical(probs)  # 创建一个离散的类别分布（Categorical Distribution），表示当前的策略πθ(a∣s)
        action = action_dist.sample()    # 根据动作的概率分布随机采样动作 at
                                         # 动作是基于当前策略的随机行为，而不是直接选择最大概率的动作，这种随机性是策略梯度方法的核心，旨在探索动作空间
        return action.item()

    def update(self, transition_dict): # 基于每局采样到的数据，通过策略梯度公式更新策略网络，使得策略更倾向于选择能够获得更高回报的动作
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']

        G = 0   # 累计折扣回报
        self.optimizer.zero_grad()
        for i in reversed(range(len(reward_list))):  # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]],
                                 dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = reward + self.gamma * G       #  G_t = r_t + γ⋅G_t+1
            loss = -log_prob * G  # 负号是因为 PyTorch 中的优化器默认是 最小化损失函数，而策略梯度方法的目标是 最大化回报。
                                  # 即：最小化 − log πθ(a∣s)⋅G_t，等价于最大化 log πθ(a∣s)⋅G_t
            loss.backward()  # 反向传播计算梯度
        self.optimizer.step()  # 更新策略网络的参数

learning_rate = 1e-3
num_episodes = 1000
hidden_dim = 128
gamma = 0.98
device = torch.device("cuda") if torch.cuda.is_available() else torch.device(
    "cpu")

env_name = "CartPole-v0"
env = gym.make(env_name)
env.reset(seed=0)

torch.manual_seed(0)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
agent = REINFORCE(state_dim, hidden_dim, action_dim, learning_rate, gamma,
                  device)

return_list = []
for i in range(10):
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):
            episode_return = 0
            transition_dict = {
                'states': [],
                'actions': [],
                'next_states': [],
                'rewards': [],
                'dones': []
            }

            state = env.reset()[0]
            done = False
            while not done:
                action = agent.take_action(state)

                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated

                transition_dict['states'].append(state)
                transition_dict['actions'].append(action)
                transition_dict['next_states'].append(next_state)
                transition_dict['rewards'].append(reward)
                transition_dict['dones'].append(done)
                state = next_state
                episode_return += reward
            return_list.append(episode_return)
            agent.update(transition_dict)    # 每一局，更新一次策略。且只有一个游戏实例
            if (i_episode + 1) % 10 == 0:
                pbar.set_postfix({
                    'episode':
                    '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                    '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

```


```python
# ------------- 打印 -------------

Iteration 0: 100%|████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.50it/s, episode=100, return=30.700]
Iteration 1: 100%|███████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.87it/s, episode=200, return=141.800]
Iteration 2: 100%|███████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.46it/s, episode=300, return=168.900]
Iteration 3: 100%|███████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.51it/s, episode=400, return=172.200]
Iteration 4: 100%|███████████████████████████████████████████████████████████| 100/100 [00:18<00:00,  5.53it/s, episode=500, return=191.700]
Iteration 5: 100%|███████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.84it/s, episode=600, return=195.500]
Iteration 6: 100%|███████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.49it/s, episode=700, return=195.400]
Iteration 7: 100%|███████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.05it/s, episode=800, return=200.000]
Iteration 8: 100%|███████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.09it/s, episode=900, return=200.000]
Iteration 9: 100%|██████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.20it/s, episode=1000, return=197.900]

```


